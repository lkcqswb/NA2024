\documentclass[a4paper]{article}
%\usepackage[affil-it]{authblk}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{float}
\usepackage{ctex}
\geometry{margin=1.5cm, vmargin={0pt,1cm}}
\setlength{\topmargin}{-1cm}
\setlength{\paperheight}{29.7cm}
\setlength{\textheight}{25.3cm}

\begin{document}
% ==================================================
\title{Numerical Analysis Homework 5}

\author{罗开诚 3220103383
  \thanks{Electronic address: \texttt{3220103383@zju.edu.com}}}
%\affil{(Information and Computational Science 2201), Zhejiang University}

\date{Due time: \today}

\maketitle

\begin{abstract}
    Solutions to various numerical analysis problems.
\end{abstract}

% ============================================

\section*{I. Provide a detailed proof of Theorem 5.7}

Verify one by one whether there exist vector addition and scalar multiplication that satisfy the following axioms.
Consider this function space \( A \):
\begin{enumerate}
    \item[(VSA-1)] Commutativity:
    \[
    \forall u(x), v(x) \in A, \quad u(x) + v(x) = v(x) + u(x).
    \]
    
    \item[(VSA-2)] Associativity:
    \[
    \forall u(x), v(x), w(x) \in A, \quad (u(x) + v(x)) + w(x) = u(x) + (v(x) + w(x)).
    \]
    
    \item[(VSA-3)] Compatibility:
    \[
    \forall c, d \in C, \quad (cd)u(x) = c(du(x)).
    \]
    
    \item[(VSA-4)] Additive identity:
    \[
    \exists 0 \in A, \text{let } O(x)=0, \quad \forall u \in V, \quad u(x) + 0 = u(x).
    \]
    
    \item[(VSA-5)] Additive inverse:
    \[
    \forall u(x) \in A, \quad \exists v(x) = -u(x) \in A, \quad u + v = 0(x).
    \]
    
    \item[(VSA-6)] Multiplicative identity:
    \[
    \exists 1 \in C, \forall u(x) \in A, \quad 1u(x) = u(x).
    \]
    
    \item[(VSA-7)] Distributive laws:
    \[
    \forall u(x), v(x) \in A, \quad \forall c, d \in C, \quad 
    \begin{cases}
        (c+d)u(x) = cu(x) + du(x), \\
        c(u(x) + v(x)) = cu(x) + cv(x).
    \end{cases}
    \]
\end{enumerate}
Thus, this is a linear space. \\
Continuing with Definition B.165, it must satisfy:
\begin{enumerate}
    \item[(IP-1)] Real positivity: 
    \[
    \forall v(x) \in A, \quad \langle v(x), v(x) \rangle=\int_{a}^{b} \rho(x)v(x)\bar{v(x)} dx \geq 0;
    \]
    \item[(IP-2)] Definiteness: 
    \[
    \langle v(x), v(x) \rangle=\int_{a}^{b} \rho(x)v(x)\bar{v(x)} dx \geq 0, v(x)\bar{v(x)} \geq 0, \rho(x) \geq 0, \iff v = 0;
    \]
    \item[(IP-3)] Additivity in the first slot:
    \[
    \forall u, v, w \in A, \quad \langle u, v + w \rangle=\int_{a}^{b} \rho(x)u(x)(v(x)+w(x))=\int_{a}^{b} \rho(x)u(x)(v(x))+\int_{a}^{b} \rho(x)u(x)(w(x)) = \langle u, v \rangle + \langle u, w \rangle;
    \]
    \item[(IP-4)] Homogeneity in the first slot:
    \[
    \forall c \in C, \quad \forall v(x), w(x) \in A, \quad \langle c v(x), w(x) \rangle=\int_{a}^{b} \rho(x)cv(x)\bar{w(x)} dx=c\int_{a}^{b} \rho(x)v(x)\bar{w(x)} dx = c \langle v(x), w(x) \rangle;
    \]
    \item[(IP-5)] Conjugate symmetry:
    \[
    \forall u(x), v(x) \in A, \quad 
    \langle u, v \rangle = \int_{a}^{b} \rho(x) u(x) \overline{v(x)} \, dx = \overline{\int_{a}^{b} \overline{\rho(x)} v(x) \overline{u(x)} \, dx} = \overline{\langle v, u \rangle}.
    \]
\end{enumerate}

Hence, this is a valid inner product. 
For Definition B.170:
\textbf{Definition B.170.} Let \( F \) be the underlying field of an inner product space \( V \). The norm induced by an inner product on \( V \) is a function \( V \to F \):

\[
\|v\| = \sqrt{\langle v, v \rangle}.
\]
\[
\|u\|_2 = \sqrt{\langle u, u \rangle} = \sqrt{\int_{a}^{b} \rho(x) u(x)\overline{u(x)} \, dx}.
\]

Thus, this is a valid norm.

\section*{II. Consider the Chebyshev polynomials of the first kind}

(a):  
The Chebyshev polynomial is \( T_n(x) = \cos(n\arccos(x)) \) on \([-1,1]\), with \( \rho(x) = \frac{1}{\sqrt{1-x^2}} \).  
The orthogonality can be derived as follows:  
\[
\int_{-1}^{1} \rho(x) T_n(x) \overline{T_m(x)} \, dx = \int_{-1}^{1} \frac{\cos(n \arccos(x)) \cos(m \arccos(x))}{\sqrt{1-x^2}} \, dx.
\]
Let \( x = \cos(t) \), and after substitution:  
\[
\int_{-1}^{1} \rho(x) T_n(x) \overline{T_m(x)} \, dx = \int_{0}^{\pi} \cos(nt) \cos(mt) \, dt.
\]
When \( m \neq n \):  
\[
\int_{0}^{\pi} \cos(nt) \cos(mt) \, dt = \frac{1}{2} \int_{0}^{\pi} \cos((m+n)t) + \cos((m-n)t) \, dt = 0.
\]
Hence, the Chebyshev polynomials are orthogonal.

(b):  
Normalize the first three Chebyshev polynomials:  
When \( n=0 \),  
\[
\int_{-1}^{1} \rho(x) T_n(x) \overline{T_n(x)} \, dx = \pi.
\]
For \( n > 0 \),  
\[
\int_{-1}^{1} \rho(x) T_n(x) \overline{T_n(x)} \, dx = \frac{\pi}{2}.
\]
Thus, multiply the coefficients by \( \frac{2}{\pi} \):  
\[
T_0'(x) = \frac{1}{\pi}, \, T_1'(x) = \frac{2}{\pi}x, \, T_2'(x) = \frac{2}{\pi}(2x^2-1), \, T_3'(x) = \frac{2}{\pi}(4x^3-3x).
\]
\section*{III.}
(a)  
When \(\rho(x) = \frac{1}{\sqrt{1-x^2}}\), the orthogonal basis should be Chebyshev polynomials.  
From Theorem 5.30, the best approximation can be expressed as \(\Sigma_{i=0}^{\infty}a_iT_i(x)\), where  
\[
a_i = \frac{\int_{-1}^{1} \frac{1}{\sqrt{1-x^2}} \cdot \sqrt{1-x^2} T_n(x) \, dx}{\|T_n(x)\|}.
\]  
For \(n=0\):  
\[
a_0 = \frac{\int_{-1}^{1} 1 \, dx}{\pi} = \frac{2}{\pi}.
\]  
For \(n=1\):  
\[
a_1 = \frac{\int_{-1}^{1} 2x \, dx}{\pi} = 0.
\]  
For \(n=2\):  
\[
a_2 = \frac{\int_{-1}^{1} 2(2x^2-1) \, dx}{\pi} = -\frac{4}{3\pi}.
\]  
Thus, the approximation result is \(\frac{2}{\pi}T_0(x) - \frac{4}{3\pi}T_2(x)\).

(b)  
From Theorem 5.6, the existence of the best approximation is guaranteed.  
Using Corollary 5.36 for necessity:  
For \(v(x) = 1, x, x^2\), all satisfy \(\langle \sqrt{1-x^2} - u(x), v(x) \rangle = 0\).  
Let \(u(x) = a_0 + a_1x + a_2x^2\).  
Then \(\sqrt{1-x^2} - u(x) = \sqrt{1-x^2} - a_0 - a_1x - a_2x^2\).  
Substitute into the equations:  
\[
\langle \sqrt{1-x^2} - a_0 - a_1x - a_2x^2, 1 \rangle = 0,
\]  
\[
\langle \sqrt{1-x^2} - a_0 - a_1x - a_2x^2, x \rangle = 0,
\]  
\[
\langle \sqrt{1-x^2} - a_0 - a_1x - a_2x^2, x^2 \rangle = 0.
\]  
Solving these equations gives the unique solution \(-\frac{8}{3\pi}x^2 + \frac{10}{3\pi}\).  
Combined with the existence theorem, this is the best approximation.

\section*{IV.}
(a)  
The first polynomial is \(u_1(x) = 1\), satisfying \(\langle u(x), u(x) \rangle = 12\), \(\|u(x)\|_2 = 2\sqrt{3}\), and \(u_1^*(x) = \frac{1}{2\sqrt{3}}\).  

The second polynomial is \(u_2(x) = x\), satisfying  
\[
v_2(x) = u_2(x) - \langle u_2(x), u_1^*(x) \rangle u_1^*(x) = x - \frac{13}{2}.
\]  
\[
\|v_2(x)\|_2 = \sqrt{143}, \quad u_2^*(x) = \frac{x - 6.5}{\sqrt{143}}.
\]  

The third polynomial is \(u_3(x) = x^2\), satisfying  
\[
v_3(x) = u_3(x) - \langle u_3(x), u_1^*(x) \rangle u_1^*(x) - \langle u_3(x), u_2^*(x) \rangle u_2^*(x) = x^2 - 13x + \frac{91}{3}.
\]  
\[
\|v_3(x)\|_3 = \sqrt{\frac{4004}{3}}, \quad u_3^*(x) = \frac{x^2 - 13x + \frac{91}{3}}{\sqrt{\frac{4004}{3}}}.
\]  

(b)  
Here, \(u_1^*(x), u_2^*(x), u_3^*(x)\) form a set of orthonormal basis.  
The fitting target \(v(x)\) can be expressed in the space spanned by \(\{1, x, x^2\}\) as  
\[
a u_1^*(x) + b u_2^*(x) + c u_3^*(x).
\]  
Using Theorem 5.30, the coefficients \(a, b, c\) can be computed as:  
\[
a = \langle v(x), u_1^*(x) \rangle = \Sigma_{i=0}^{12} y_i \cdot u_1^*(x_i) = 479.778,
\]  
\[
b = \langle v(x), u_2^*(x) \rangle = \Sigma_{i=0}^{12} y_i \cdot u_2^*(x_i) = 49.25465,
\]  
\[
c = \langle v(x), u_3^*(x) \rangle = \Sigma_{i=0}^{12} y_i \cdot u_3^*(x_i) = 330.33.
\]  
The computed coefficients are \(a_0 = 385.99942\), \(a_1 = -113.4263\), \(a_2 = 9.0419\), which are similar to the provided answers.

(c)  
The computation of the orthogonal basis can be reused, while the part for computing each basis coefficient needs to be recalculated.  
Using orthogonal bases requires only three inner products to obtain the expression.  
However, using normal equations involves both inner product computations and an additional linear transformation.

\section*{V. Prove Theorem 5.66 and Lemma 5.67.}
(PDI-1)\\
For any \( y \in F^n / \text{ker}(A) \)\
\[ AA^+A y = A \Sigma_{j=1}^r \frac{1}{\sigma_j} \langle Ay, v_j \rangle u_j \]
\[ = A \Sigma_{j=1}^r \frac{1}{\sigma_j} \langle Ay, v_j \rangle u_j = \Sigma_{j=1}^r \langle Ay, v_j \rangle \frac{1}{\sigma_j} Au_j = \Sigma_{j=1}^r \langle Ay, v_j \rangle v_j = Ay \]
Thus, for any \( y \in F^n / \text{ker}(A) \)\
\[ (AA^+A - A)y = 0 \]
\[ AA^+A = A \]
(PDI-2)\\
For any \( y \in F^m / \text{ker}(A^*)^* \)\
\[ A^+AA^+ y = A^+ A \Sigma_{j=1}^r \frac{1}{\sigma_j} \langle y, v_j \rangle u_j \]
\[ = A^+ \Sigma_{j=1}^r \langle y, v_j \rangle v_j = A^+ y \]
Similarly, for any \( y \in F^m / \text{ker}(A^*)^* \)\
\[ (A^+AA^+ - A^+)y = 0 \]
\[ (A^+AA^+ = A^+)y \]

(PDI-3)\\
For any \( y_1, y_2 \in F^m / \text{ker}(A^*)^* \)
\[ \langle AA^+ y_1, y_2 \rangle = \langle A \Sigma_{j=1}^r \frac{1}{\sigma_j} \langle Ay_1, v_j \rangle u_j, y_2 \rangle = \langle \Sigma_{j=1}^r \langle y_1, v_j \rangle v_j, y_2 \rangle \]
Considering \( y \) is in the span of \( v_1, \ldots, v_r \), \( \langle AA^+ y_1, y_2 \rangle = \langle \Sigma_{j=1}^r \langle y_1, v_j \rangle v_j, \Sigma_{j=1}^r \langle y_2, v_j \rangle v_j \rangle \)
Similarly
\[ \langle (AA^+)^* y_1, y_2 \rangle = \langle y_1, (AA^+) y_2 \rangle = \langle y_1, \Sigma_{j=1}^r \langle y_2, v_j \rangle v_j \rangle = \langle \Sigma_{j=1}^r \langle y_1, v_j \rangle v_j, \Sigma_{j=1}^r \langle y_2, v_j \rangle v_j \rangle \]
Thus, we have \( (AA^+)^* = AA^+ \)
\[ A^+A = A^+ V \Sigma U = (A^+ v_1, A^+ v_2, \ldots) \Sigma U^* \]
\[ = (\Sigma_{j=1}^r \frac{1}{\sigma_j} \langle v_1, v_j \rangle u_j, \ldots) \Sigma U^* \]
\[ = (u_1, u_2, \ldots) U^* \]
\[ (A^+A)^* = ((u_1, u_2, \ldots, 0, 0, \ldots) (u_1, u_2, \ldots)^*)^* = (u_1, u_2, \ldots) (u_1, u_2, \ldots, 0, 0, \ldots)^* = (u_1, u_2, \ldots, 0, 0, 0) (u_1, u_2, \ldots)^* = A^+A \] is established.

5.67\\
If the columns are full rank, \( r = m \),\\
By PDI-1, \( A^* = (AA^+A)^* = A^* (AA^+)^* \), by PDI-3\\
\[ A^* (AA^+)^* = A^* AA^+ = (A^* A) A^+ \]
\( A \) is full column rank, \( (A^* A) \) is a full rank matrix and invertible.\\
\[ (A^* A)^{-1} A^* = A^+ \]
Similarly, when the rows are full rank \( r = n \)\\
By PDI-1, \( A^* = (AA^+A)^* = (A^+ A)^* A^* \), by PDI-3\\
\[ (A^+ A)^* A^* = (A^+ A) A^* = A^+ (AA^*) \]
\( AA^* \) is full rank and invertible.\\
\[ A^* (AA^*)^{-1} = A^+ \]




\section*{\center{\normalsize{Acknowledgement}}}
This document was created using GPT-4 for rapid template transformation and Kimi AI for English grammar correction.

\end{document}
